{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "# cwd = os.getcwd()\n",
    "# if not os.path.exists(str(cwd + \"/data/\")):\n",
    "#     os.makedirs(str(cwd + \"/data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def WordImageGenerator(words, word_font, image_name, image_size = (320, 80), offest = (40, 10)):\n",
    "    '''\n",
    "    Generate an image of text\n",
    "    word:      The text to display in the image\n",
    "    word_font:      The font to use\n",
    "    image_name:     The file name\n",
    "    image_size:      The image size\n",
    "    offest:      The offest of the text in the image\n",
    "    '''\n",
    "    img = Image.new('RGB', image_size, \"black\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Draw.text(xy, text, color_fill=None, font=None, anchor=None)\n",
    "    draw.text(offest, words, (255, 255, 255), font = word_font)\n",
    "    # img.save('/absolute/path/to/myphoto.jpg', 'JPEG')\n",
    "    img.save(image_name, 'JPEG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # The possible words to add in the image\n",
    "url = 'https://www.randomlists.com/data/words.json'\n",
    "response = requests.get(url).json()\n",
    "canditate_words_list = response[\"data\"]\n",
    "print(canditate_words_list[:10])\n",
    "print(len(canditate_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image = 8\n",
    "num_words_per_image = list(np.random.randint(1, 3, size = num_image))\n",
    "words_list = [' '.join(np.random.choice(canditate_words_list, num)) for num in num_words_per_image]\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the font of word\n",
    "# Linux\n",
    "# font = ImageFont.truetype(\"LiberationMono-Regular.ttf\", 16)\n",
    "# font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "# Mac \n",
    "font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 26)\n",
    "\n",
    "#The largest size of word needed\n",
    "max_font_size = max(font.getsize(words) for words in words_list)\n",
    "print(max_font_size)\n",
    "#Computed offset\n",
    "offset = ((320 - max_font_size[0]) // 2, (80 - max_font_size[1]) // 2)\n",
    "print(offset)\n",
    "#Image size\n",
    "image_size = (320, 80)\n",
    "record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for image_id, words in enumerate(words_list):\n",
    "    WordImageGenerator(words, font, str(image_id) + '.png', image_size, offset)\n",
    "    record.append(str(image_id) + '.png,' + words)\n",
    "    \n",
    "#Write CSV file\n",
    "with open('Train.csv', 'w') as file:\n",
    "    file.write('\\n'.join(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Until, the 'traing image' i.e., An English word image generators, is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from TFANN import ANNC\n",
    "# TFANN.py needs module named 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData(file_path = '.'):\n",
    "    '''\n",
    "    Loads the OCR dataset. A is matrix of images (NIMG, Height, Width, Channel).\n",
    "    Y is matrix of characters (NIMG, MAX_CHAR)\n",
    "    file_path:     Path to OCR data folder\n",
    "    return: Data Matrix, Target Matrix, Target Strings\n",
    "    '''\n",
    "    train_file_path = os.path.join(file_path, 'Train.csv')\n",
    "    A, Y, T, FN = [], [], [], []\n",
    "    with open(train_file_path) as file:\n",
    "        for Li in file:\n",
    "            FNi, Yi = Li.strip().split(',')                     #filename,string\n",
    "            T.append(Yi)\n",
    "            A.append(imread(os.path.join(file_path, 'Out', FNi)))\n",
    "            Y.append(list(Yi) + [' '] * (MAX_CHAR - len(Yi)))   #Pad strings with spaces\n",
    "            FN.append(FNi)\n",
    "    return np.stack(A), np.stack(Y), np.stack(T), np.stack(FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the neural network is constructed using the artificial neural network classifier (ANNC) class from TFANN. The architecture described above is represented in the following lines of code using ANNC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the neural network\n",
    "NC = len(string.ascii_letters + string.digits + ' ')\n",
    "MAX_CHAR = 64\n",
    "IS = (14, 640, 3)       #Image size for CNN\n",
    "ws = [('C', [4, 4,  3, NC // 2], [1, 2, 2, 1]), ('AF', 'relu'), \n",
    "      ('C', [4, 4, NC // 2, NC], [1, 2, 1, 1]), ('AF', 'relu'), \n",
    "      ('C', [8, 5, NC, NC], [1, 8, 5, 1]), ('AF', 'relu'),\n",
    "      ('R', [-1, 64, NC])]\n",
    "#Create the neural network in TensorFlow\n",
    "cnnc = ANNC(IS, ws, batchSize = 64, learnRate = 5e-5, maxIter = 32, reg = 1e-5, tol = 1e-2, verbose = True)\n",
    "if not cnnc.RestoreModel('TFModel/', 'ocrnet'):\n",
    "    #Fit the network\n",
    "    cnnc.fit(A, Y)\n",
    "    #The predictions as sequences of character indices\n",
    "    YH = np.zeros((Y.shape[0], Y.shape[1]), dtype = np.int)\n",
    "    for i in np.array_split(np.arange(A.shape[0]), 32): \n",
    "        YH[i] = np.argmax(cnnc.predict(A[i]), axis = 2)\n",
    "    #Convert from sequence of char indices to strings\n",
    "    PS = [''.join(CS[j] for j in YHi) for YHi in YH]\n",
    "    for PSi, Ti in zip(PS, T):\n",
    "        print(Ti + '\\t->\\t' + PSi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
